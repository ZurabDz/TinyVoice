{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4672f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from conformer.tokenizer import Tokenizer\n",
    "from conformer.dataset import batch_fn, ProcessAudioData, unpack_speech_data\n",
    "import grain\n",
    "from pathlib import Path\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from conformer.model import ConformerModel\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "\n",
    "tokenizer = Tokenizer.load_tokenizer(Path('/home/penguin/data/tinyvoice/tokenizer/tokenizer.pkl'))\n",
    "\n",
    "\n",
    "train_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/train.array_record')\n",
    "test_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/test.array_record')\n",
    "\n",
    "\n",
    "\n",
    "map_train_audio_dataset = grain.MapDataset.source(train_audio_source)\n",
    "map_test_audio_dataset = grain.MapDataset.source(test_audio_source)\n",
    "\n",
    "\n",
    "batch_size = 48\n",
    "steps_per_epoch = len(map_train_audio_dataset) // batch_size\n",
    "num_epochs = 5\n",
    "\n",
    "processed_train_dataset = (\n",
    "    map_train_audio_dataset\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=batch_size, batch_fn=batch_fn)\n",
    "    .repeat(num_epochs)\n",
    ")\n",
    "\n",
    "processed_test_dataset = (\n",
    "    map_test_audio_dataset\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=8, batch_fn=batch_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64f9dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "small = processed_train_dataset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92c3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerModel(token_count=len(tokenizer.id_to_char))\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=1e-7,\n",
    "    peak_value=5e-4,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=10000,\n",
    "    end_value=1e-6\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.98,\n",
    "        weight_decay=1e-2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447dfb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytree = {'model': nnx.state(model)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb03b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ocp.CheckpointManagerOptions(max_to_keep=5, save_interval_steps=100)\n",
    "manager = ocp.CheckpointManager(ocp.test_utils.erase_and_create_empty('/home/penguin/Desktop/TinyVoice/checkpoints'), options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988ca639",
   "metadata": {},
   "outputs": [],
   "source": [
    "manager.save(2, args=ocp.args.StandardSave(pytree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d9a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "restored = manager.restore(2, args=ocp.args.StandardRestore(pytree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e8cce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.update(model, restored['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e539b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if checkpointer.latest_step() is not None:\n",
    "    latest_step = checkpointer.latest_step()\n",
    "    print(f\"Restoring from checkpoint at step {latest_step}...\")\n",
    "    restored = checkpointer.restore(latest_step)\n",
    "    nnx.update(model, restored['model'])\n",
    "    nnx.update(optimizer, restored['optimizer'])\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    def loss_fn(model):\n",
    "        logits = model(padded_audios, mask=mask, training=True)\n",
    "        \n",
    "        audio_time_mask = jnp.arange(logits.shape[1]) >= real_times[:, None]\n",
    "        label_mask = jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]\n",
    "        \n",
    "        loss = optax.ctc_loss(logits, audio_time_mask, padded_labels, label_mask).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@nnx.jit\n",
    "def jitted_validation(model, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    logits = model(padded_audios, mask=mask, training=False)\n",
    "    \n",
    "    audio_time_mask = jnp.arange(logits.shape[1]) >= real_times[:, None]\n",
    "    label_mask = jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]\n",
    "    \n",
    "    loss = optax.ctc_loss(logits, audio_time_mask, padded_labels, label_mask).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[0]\n",
    "\n",
    "\n",
    "#\n",
    "def compute_mask(frames):\n",
    "    # MelSpectrogram: hop_length=160, win_length=400, padded=False\n",
    "    # T_mel = (T_audio - win_length) // hop_length + 1\n",
    "    # Conv2dSubSampler: two layers of kernel=3, stride=2, padding='VALID'\n",
    "    # T_out = (T_in - 3) // 2 + 1\n",
    "    # T_final = (T_out - 3) // 2 + 1\n",
    "    \n",
    "    t_mel = (frames - 400) // 160 + 1\n",
    "    t_conv1 = (t_mel - 3) // 2 + 1\n",
    "    t_final = (t_conv1 - 3) // 2 + 1\n",
    "    \n",
    "    max_frames = 235008\n",
    "    max_t_mel = (max_frames - 400) // 160 + 1\n",
    "    max_t_conv1 = (max_t_mel - 3) // 2 + 1\n",
    "    max_t_final = (max_t_conv1 - 3) // 2 + 1\n",
    "\n",
    "    real_times = t_final\n",
    "    \n",
    "    # Square mask for attention\n",
    "    mask = jnp.arange(max_t_final) < real_times[:, None]\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(max_t_final, axis=1)\n",
    "    \n",
    "    # MultiHeadAttention mask: (batch, num_heads, q_len, k_len)\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(4, axis=1)\n",
    "\n",
    "    return mask, real_times\n",
    "\n",
    "\n",
    "mask, real_times = compute_mask(frames)\n",
    "\n",
    "\n",
    "z = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "\n",
    "\n",
    "avg_loss = 0\n",
    "global_step = 0\n",
    "\n",
    "if checkpointer.latest_step() is not None:\n",
    "    global_step = checkpointer.latest_step()\n",
    "\n",
    "print(f\"Steps per epoch: {steps_per_epoch}\")\n",
    "\n",
    "for i, element in enumerate(tqdm(processed_train_dataset)):\n",
    "    padded_audios, frames, padded_labels, label_lengths = element\n",
    "    mask, real_times = compute_mask(frames)\n",
    "\n",
    "    loss = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "\n",
    "    avg_loss += loss\n",
    "    global_step += 1\n",
    "    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"Step {global_step}, Train Loss: {avg_loss / 20:.4f}\")\n",
    "        avg_loss = 0\n",
    "\n",
    "    # Validation and Checkpointing at the end of each epoch\n",
    "    if (i + 1) % steps_per_epoch == 0:\n",
    "        epoch = (i + 1) // steps_per_epoch\n",
    "        print(f\"\\nEnd of Epoch {epoch}. Running validation...\")\n",
    "        \n",
    "        val_loss = 0\n",
    "        val_steps = 0\n",
    "        for val_element in tqdm(processed_test_dataset, desc=\"Validation\"):\n",
    "            v_padded_audios, v_frames, v_padded_labels, v_label_lengths = val_element\n",
    "            v_mask, v_real_times = compute_mask(v_frames)\n",
    "            \n",
    "            v_loss = jitted_validation(model, v_padded_audios, v_padded_labels, v_mask, v_real_times, v_label_lengths)\n",
    "            val_loss += v_loss\n",
    "            val_steps += 1\n",
    "        \n",
    "        if val_steps > 0:\n",
    "            avg_val_loss = val_loss / val_steps\n",
    "            print(f\"Epoch {epoch} Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        # Checkpointing\n",
    "        print(f\"Saving checkpoint at step {global_step}...\")\n",
    "        checkpointer.save(\n",
    "            global_step,\n",
    "            {\n",
    "                'model': nnx.state(model),\n",
    "                'optimizer': nnx.state(optimizer),\n",
    "            }\n",
    "        )\n",
    "        checkpointer.wait_until_finished()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
