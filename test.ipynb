{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d9408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from conformer.tokenizer import Tokenizer\n",
    "from conformer.dataset import batch_fn, ProcessAudioData, unpack_speech_data\n",
    "import grain\n",
    "from pathlib import Path\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from conformer.model import ConformerEncoder\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import os\n",
    "# os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67d02aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0102 23:40:25.149051  105155 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W0102 23:40:25.154219  104824 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = Path('/home/penguin/Documents/TinyVoice/checkpoint/checkpoints_fixed')\n",
    "\n",
    "checkpointer = ocp.CheckpointManager(\n",
    "    checkpoint_path.absolute(),\n",
    "    options=ocp.CheckpointManagerOptions(max_to_keep=5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6eefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load_tokenizer(Path('/home/penguin/data/ka/tokenizer/tokenizer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8816aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerEncoder(token_count=len(tokenizer.id_to_char))\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=1e-7,\n",
    "    peak_value=5e-4,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=10000,\n",
    "    end_value=1e-6\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.98,\n",
    "        weight_decay=1e-2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75977db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if checkpointer.latest_step() is not None:\n",
    "    latest_step = checkpointer.latest_step()\n",
    "    print(f\"Restoring from checkpoint at step {latest_step}...\")\n",
    "    restored = checkpointer.restore(latest_step)\n",
    "    nnx.update(model, restored['model'])\n",
    "    # nnx.update(optimizer, restored['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a075037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/train.array_record')\n",
    "test_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/test.array_record')\n",
    "\n",
    "\n",
    "map_train_audio_dataset = grain.MapDataset.source(train_audio_source)\n",
    "map_test_audio_dataset = grain.MapDataset.source(test_audio_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2561b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_dataset = (\n",
    "    map_train_audio_dataset\n",
    "    .shuffle(seed=42)\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=48, batch_fn=batch_fn)\n",
    "    .repeat(1)\n",
    ")\n",
    "\n",
    "processed_test_dataset = (\n",
    "    map_test_audio_dataset\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=24, batch_fn=batch_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd43b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mask(frames):\n",
    "    # MelSpectrogram: hop_length=160, win_length=400, padded=False\n",
    "    # T_mel = (T_audio - win_length) // hop_length + 1\n",
    "    # Conv2dSubSampler: two layers of kernel=3, stride=2, padding='VALID'\n",
    "    # T_out = (T_in - 3) // 2 + 1\n",
    "    # T_final = (T_out - 3) // 2 + 1\n",
    "    \n",
    "    t_mel = (frames - 400) // 160 + 1\n",
    "    t_conv1 = (t_mel - 3) // 2 + 1\n",
    "    t_final = (t_conv1 - 3) // 2 + 1\n",
    "    \n",
    "    max_frames = 235008\n",
    "    max_t_mel = (max_frames - 400) // 160 + 1\n",
    "    max_t_conv1 = (max_t_mel - 3) // 2 + 1\n",
    "    max_t_final = (max_t_conv1 - 3) // 2 + 1\n",
    "\n",
    "    real_times = t_final\n",
    "    \n",
    "    # Square mask for attention\n",
    "    mask = jnp.arange(max_t_final) < real_times[:, None]\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(max_t_final, axis=1)\n",
    "    \n",
    "    # MultiHeadAttention mask: (batch, num_heads, q_len, k_len)\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(4, axis=1)\n",
    "\n",
    "    return mask, real_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[12]\n",
    "mask, real_times = compute_mask(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a03b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model(padded_audios, mask, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    def loss_fn(model):\n",
    "        logits = model(padded_audios, mask=mask, training=True)\n",
    "        \n",
    "        audio_time_mask = jnp.arange(logits.shape[1]) >= real_times[:, None]\n",
    "        label_mask = jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]\n",
    "        \n",
    "        loss = optax.ctc_loss(logits, audio_time_mask, padded_labels, label_mask, blank_id=tokenizer.blank_id).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb901933",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, real_times = compute_mask(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ebf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with jax.profiler.trace('./profiler/jax-trace') as profiler:\n",
    "avg_loss = 0\n",
    "for i, element in enumerate(tqdm(processed_train_dataset)):\n",
    "    padded_audios, frames, padded_labels, label_lengths = element\n",
    "    mask, real_times = compute_mask(frames)\n",
    "\n",
    "    loss = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "\n",
    "    avg_loss += loss\n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"avg loss: {avg_loss // 20}\")\n",
    "        avg_loss = 0\n",
    "\n",
    "        # if i == 4:\n",
    "        #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062996e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> str:\n",
    "    last_char_id = 0\n",
    "    decoded_chars = []\n",
    "    for char_id in ids:\n",
    "        if char_id != 0 and char_id != last_char_id:\n",
    "            decoded_chars.append(char_id)\n",
    "        last_char_id = char_id\n",
    "    \n",
    "    return decoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = decode(output[4].argmax(axis=-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3092e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.decode(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c59977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.decode(output[10].argmax(axis=-1).tolist())\n",
    "for tok in tokens:\n",
    "    print(tok, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tokenizer.decode(padded_labels[4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in z:\n",
    "    if tok != '<BLANK>':\n",
    "        print(tok, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
