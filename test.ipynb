{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d9408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from conformer.tokenizer import Tokenizer\n",
    "from conformer.dataset import batch_fn, ProcessAudioData, unpack_speech_data\n",
    "import grain\n",
    "from pathlib import Path\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from conformer.model import ConformerModel\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "import orbax.checkpoint as ocp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d02aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1220 22:54:25.379513  188213 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "W1220 22:54:25.381498  188050 cuda_executor.cc:1802] GPU interconnect information not available: INTERNAL: NVML doesn't support extracting fabric info or NVLink is not used by the device.\n",
      "WARNING:absl:Configured `CheckpointManager` using deprecated legacy API. Please follow the instructions at https://orbax.readthedocs.io/en/latest/guides/checkpoint/api_refactor.html to migrate.\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = Path('/home/penguin/data/ka/checkpoints')\n",
    "checkpointer = ocp.CheckpointManager(\n",
    "    checkpoint_path.absolute(),\n",
    "    ocp.PyTreeCheckpointer(),\n",
    "    options=ocp.CheckpointManagerOptions(max_to_keep=5, save_interval_steps=100)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a6eefb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load_tokenizer(Path('/home/penguin/data/tinyvoice/tokenizer/tokenizer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8816aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerModel(token_count=len(tokenizer.id_to_char))\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=1e-7,\n",
    "    peak_value=5e-4,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=10000,\n",
    "    end_value=1e-6\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.98,\n",
    "        weight_decay=1e-2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75977db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring from checkpoint at step 1310...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/penguin/Desktop/TinyVoice/.venv/lib/python3.12/site-packages/orbax/checkpoint/_src/serialization/jax_array_handlers.py:711: UserWarning: Sharding info not provided when restoring. Populating sharding info from sharding file. Please note restoration time will be slightly increased due to reading from file. Note also that this option is unsafe when restoring on a different topology than the checkpoint was saved with.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if checkpointer.latest_step() is not None:\n",
    "    latest_step = checkpointer.latest_step()\n",
    "    print(f\"Restoring from checkpoint at step {latest_step}...\")\n",
    "    restored = checkpointer.restore(latest_step)\n",
    "    nnx.update(model, restored['model'])\n",
    "    # nnx.update(optimizer, restored['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a716ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load_tokenizer(Path('/home/penguin/data/tinyvoice/tokenizer/tokenizer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a075037",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/train.array_record')\n",
    "test_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/test.array_record')\n",
    "\n",
    "\n",
    "map_train_audio_dataset = grain.MapDataset.source(train_audio_source)\n",
    "map_test_audio_dataset = grain.MapDataset.source(test_audio_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2561b69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_train_dataset = (\n",
    "    map_train_audio_dataset\n",
    "    .shuffle(seed=42)\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=48, batch_fn=batch_fn)\n",
    "    .repeat(5)\n",
    ")\n",
    "\n",
    "processed_test_dataset = (\n",
    "    map_test_audio_dataset\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=24, batch_fn=batch_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccd43b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def compute_mask(frames):\n",
    "    # MelSpectrogram: hop_length=160, win_length=400, padded=False\n",
    "    # T_mel = (T_audio - win_length) // hop_length + 1\n",
    "    # Conv2dSubSampler: two layers of kernel=3, stride=2, padding='VALID'\n",
    "    # T_out = (T_in - 3) // 2 + 1\n",
    "    # T_final = (T_out - 3) // 2 + 1\n",
    "    \n",
    "    t_mel = (frames - 400) // 160 + 1\n",
    "    t_conv1 = (t_mel - 3) // 2 + 1\n",
    "    t_final = (t_conv1 - 3) // 2 + 1\n",
    "    \n",
    "    max_frames = 235008\n",
    "    max_t_mel = (max_frames - 400) // 160 + 1\n",
    "    max_t_conv1 = (max_t_mel - 3) // 2 + 1\n",
    "    max_t_final = (max_t_conv1 - 3) // 2 + 1\n",
    "\n",
    "    real_times = t_final\n",
    "    \n",
    "    # Square mask for attention\n",
    "    mask = jnp.arange(max_t_final) < real_times[:, None]\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(max_t_final, axis=1)\n",
    "    \n",
    "    # MultiHeadAttention mask: (batch, num_heads, q_len, k_len)\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(4, axis=1)\n",
    "\n",
    "    return mask, real_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0e2545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[12]\n",
    "mask, real_times = compute_mask(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7a03b41",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "Ellipsis",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadded_audios\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TinyVoice/conformer/model.py:200\u001b[39m, in \u001b[36mConformerModel.__call__\u001b[39m\u001b[34m(self, x, mask, training)\u001b[39m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask, training):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m     \u001b[38;5;66;03m# return self.decoder(output)\u001b[39;00m\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TinyVoice/conformer/model.py:126\u001b[39m, in \u001b[36mConformerEncoder.__call__\u001b[39m\u001b[34m(self, x, mask, training)\u001b[39m\n\u001b[32m    124\u001b[39m x = \u001b[38;5;28mself\u001b[39m.mel_spectogram(x, training)\n\u001b[32m    125\u001b[39m x = \u001b[38;5;28mself\u001b[39m.conv_subsampler(x[:, :, :, \u001b[38;5;28;01mNone\u001b[39;00m])\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    127\u001b[39m x = \u001b[38;5;28mself\u001b[39m.pos_encoding(x)\n\u001b[32m    128\u001b[39m x = \u001b[38;5;28mself\u001b[39m.dropout(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TinyVoice/.venv/lib/python3.12/site-packages/flax/nnx/nn/linear.py:405\u001b[39m, in \u001b[36mLinear.__call__\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) -> Array:\n\u001b[32m    397\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[32m    398\u001b[39m \n\u001b[32m    399\u001b[39m \u001b[33;03m  Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    403\u001b[39m \u001b[33;03m    The transformed input.\u001b[39;00m\n\u001b[32m    404\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m405\u001b[39m   kernel = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m.\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    406\u001b[39m   bias = \u001b[38;5;28mself\u001b[39m.bias[...] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    408\u001b[39m   inputs, kernel, bias = \u001b[38;5;28mself\u001b[39m.promote_dtype(\n\u001b[32m    409\u001b[39m     (inputs, kernel, bias), dtype=\u001b[38;5;28mself\u001b[39m.dtype\n\u001b[32m    410\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TinyVoice/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:1664\u001b[39m, in \u001b[36mVariable.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m-> \u001b[39m\u001b[32m1664\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/TinyVoice/.venv/lib/python3.12/site-packages/flax/nnx/variablelib.py:1419\u001b[39m, in \u001b[36mVariable.get_value\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   1417\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# skip trivial access\u001b[39;00m\n\u001b[32m   1418\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1419\u001b[39m     value = \u001b[43mvalue\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_array_ref(value):\n\u001b[32m   1421\u001b[39m   value = value[...]\n",
      "\u001b[31mKeyError\u001b[39m: Ellipsis"
     ]
    }
   ],
   "source": [
    "res = model(padded_audios, mask, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9904ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = grain.MapDataset.range(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5b8b875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4329"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('ჩ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "31501812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7337"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('Ჩ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9b7fc20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2eb4a497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/home/penguin/data/ka/test_processed.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f18a558b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969    კაკალი ხიდან ჩამოვარდა\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['path'] == '/home/penguin/data/ka/clips_16k/common_voice_ka_39937922.flac']['label'].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a582f8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = mds.shuffle(seed=42).repeat(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5127c3d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conformer.model import ConformerModel\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b40b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerModel(token_count=len(tokenizer.id_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3229dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=1e-7,\n",
    "    peak_value=5e-4,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=10000,\n",
    "    end_value=1e-6\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.98,\n",
    "        weight_decay=1e-2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d5e09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@nnx.jit\n",
    "def jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    def loss_fn(model):\n",
    "        logits = model(padded_audios, mask=mask, training=True)\n",
    "        \n",
    "        audio_time_mask = jnp.arange(logits.shape[1]) >= real_times[:, None]\n",
    "        label_mask = jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]\n",
    "        \n",
    "        loss = optax.ctc_loss(logits, audio_time_mask, padded_labels, label_mask).mean()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da84b538",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc46fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "def compute_mask(frames):\n",
    "    # MelSpectrogram: hop_length=160, win_length=400, padded=False\n",
    "    # T_mel = (T_audio - win_length) // hop_length + 1\n",
    "    # Conv2dSubSampler: two layers of kernel=3, stride=2, padding='VALID'\n",
    "    # T_out = (T_in - 3) // 2 + 1\n",
    "    # T_final = (T_out - 3) // 2 + 1\n",
    "    \n",
    "    t_mel = (frames - 400) // 160 + 1\n",
    "    t_conv1 = (t_mel - 3) // 2 + 1\n",
    "    t_final = (t_conv1 - 3) // 2 + 1\n",
    "    \n",
    "    max_frames = 235008\n",
    "    max_t_mel = (max_frames - 400) // 160 + 1\n",
    "    max_t_conv1 = (max_t_mel - 3) // 2 + 1\n",
    "    max_t_final = (max_t_conv1 - 3) // 2 + 1\n",
    "\n",
    "    real_times = t_final\n",
    "    \n",
    "    # Square mask for attention\n",
    "    mask = jnp.arange(max_t_final) < real_times[:, None]\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(max_t_final, axis=1)\n",
    "    \n",
    "    # MultiHeadAttention mask: (batch, num_heads, q_len, k_len)\n",
    "    mask = jnp.expand_dims(mask, axis=1).repeat(4, axis=1)\n",
    "\n",
    "    return mask, real_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb901933",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask, real_times = compute_mask(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfdfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4ebf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = 0\n",
    "for i, element in enumerate(tqdm(processed_train_dataset)):\n",
    "    padded_audios, frames, padded_labels, label_lengths = element\n",
    "    mask, real_times = compute_mask(frames)\n",
    "\n",
    "    loss = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "\n",
    "    avg_loss += loss    \n",
    "    if (i + 1) % 20 == 0:\n",
    "        print(f\"avg loss: {avg_loss // 20}\")\n",
    "        avg_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77fc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[12]\n",
    "mask, real_times = compute_mask(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23822ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(padded_audios, mask=mask, training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062996e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(ids: list[int]) -> str:\n",
    "    last_char_id = 0\n",
    "    decoded_chars = []\n",
    "    for char_id in ids:\n",
    "        if char_id != 0 and char_id != last_char_id:\n",
    "            decoded_chars.append(char_id)\n",
    "        last_char_id = char_id\n",
    "    \n",
    "    return decoded_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bfadbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = decode(output[4].argmax(axis=-1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3092e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.decode(dds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c59977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.decode(output[10].argmax(axis=-1).tolist())\n",
    "for tok in tokens:\n",
    "    print(tok, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69e58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tokenizer.decode(padded_labels[4].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767dce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tok in z:\n",
    "    if tok != '<BLANK>':\n",
    "        print(tok, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b593f9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiny",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
