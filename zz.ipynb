{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11012cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from conformer.tokenizer import Tokenizer\n",
    "from conformer.dataset import batch_fn, ProcessAudioData, unpack_speech_data\n",
    "import grain\n",
    "from pathlib import Path\n",
    "from flax import nnx\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from conformer.model import ConformerEncoder\n",
    "from tqdm import tqdm\n",
    "import optax\n",
    "import orbax.checkpoint as ocp\n",
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1\"\n",
    "\n",
    "\n",
    "checkpoint_path = Path('/home/penguin/data/ka/checkpoints')\n",
    "checkpointer = ocp.CheckpointManager(\n",
    "    checkpoint_path.absolute(),\n",
    "    options=ocp.CheckpointManagerOptions(max_to_keep=5)\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.load_tokenizer(Path('/home/penguin/data/ka/tokenizer/tokenizer.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc213f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerEncoder(token_count=len(tokenizer.id_to_char))\n",
    "model = ConformerEncoder(token_count=len(tokenizer.id_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnx.display(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model = ConformerEncoder(token_count=len(tokenizer.id_to_char))\n",
    "\n",
    "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
    "    init_value=1e-7,\n",
    "    peak_value=5e-4,\n",
    "    warmup_steps=1000,\n",
    "    decay_steps=10000,\n",
    "    end_value=1e-6\n",
    ")\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=0.9,\n",
    "        b2=0.98,\n",
    "        weight_decay=1e-2\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")\n",
    "\n",
    "if checkpointer.latest_step() is not None:\n",
    "    latest_step = checkpointer.latest_step()\n",
    "    print(f\"Restoring from checkpoint at step {latest_step}...\")\n",
    "    \n",
    "    # Create abstract state for restore template\n",
    "    abstract_model = nnx.eval_shape(lambda: ConformerEncoder(token_count=len(tokenizer.id_to_char)))\n",
    "    abstract_optimizer = nnx.eval_shape(lambda: nnx.Optimizer(\n",
    "        abstract_model,\n",
    "        optax.adamw(learning_rate=lr_schedule, b1=0.9, b2=0.98, weight_decay=1e-2),\n",
    "        wrt=nnx.Param\n",
    "    ))\n",
    "    \n",
    "    restored = checkpointer.restore(\n",
    "        latest_step,\n",
    "        args=ocp.args.Composite(\n",
    "            model=ocp.args.StandardRestore(nnx.state(abstract_model)),\n",
    "            optimizer=ocp.args.StandardRestore(nnx.state(abstract_optimizer)),\n",
    "        )\n",
    "    )\n",
    "    nnx.update(model, restored.model)\n",
    "    nnx.update(optimizer, restored.optimizer)\n",
    "\n",
    "\n",
    "\n",
    "train_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/train.array_record')\n",
    "test_audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/ka/packed_dataset/test.array_record')\n",
    "\n",
    "\n",
    "map_train_audio_dataset = grain.MapDataset.source(train_audio_source)\n",
    "map_test_audio_dataset = grain.MapDataset.source(test_audio_source)\n",
    "\n",
    "\n",
    "processed_train_dataset = (\n",
    "    map_train_audio_dataset\n",
    "    .shuffle(seed=42)\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=48, batch_fn=batch_fn)\n",
    "    .repeat(1)\n",
    ")\n",
    "\n",
    "processed_test_dataset = (\n",
    "    map_test_audio_dataset\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=24, batch_fn=batch_fn)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_mask(frames):\n",
    "    # MelSpectrogram: hop_length=160, win_length=400, padded=False\n",
    "    # T_mel = (T_audio - win_length) // hop_length + 1\n",
    "    # Conv2dSubSampler: two layers of kernel=3, stride=2, padding='VALID'\n",
    "    # T_out = (T_in - 3) // 2 + 1\n",
    "    # T_final = (T_out - 3) // 2 + 1\n",
    "    \n",
    "    t_mel = (frames - 400) // 160 + 1\n",
    "    t_conv1 = (t_mel - 3) // 2 + 1\n",
    "    t_final = (t_conv1 - 3) // 2 + 1\n",
    "    \n",
    "    max_frames = 235008\n",
    "    max_t_mel = (max_frames - 400) // 160 + 1\n",
    "    max_t_conv1 = (max_t_mel - 3) // 2 + 1\n",
    "    max_t_final = (max_t_conv1 - 3) // 2 + 1\n",
    "\n",
    "    real_times = t_final\n",
    "    \n",
    "    # 1D mask: True for valid positions, False for padding\n",
    "    valid_mask = jnp.arange(max_t_final) < real_times[:, None]  # (batch, max_t_final)\n",
    "    \n",
    "    # For MultiHeadAttention: (batch, num_heads, q_len, k_len)\n",
    "    # True = attend, False = mask out\n",
    "    # We want to mask keys that are padding, so expand along query dimension\n",
    "    attention_mask = valid_mask[:, None, None, :]  # (batch, 1, 1, k_len)\n",
    "    attention_mask = jnp.broadcast_to(attention_mask, (valid_mask.shape[0], 4, max_t_final, max_t_final))\n",
    "\n",
    "    return attention_mask, real_times\n",
    "\n",
    "\n",
    "padded_audios, frames, padded_labels, label_lengths = processed_train_dataset[12]\n",
    "mask, real_times = compute_mask(frames)\n",
    "\n",
    "loss = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "\n",
    "@nnx.jit\n",
    "def jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    \"\"\"Training step with gradient computation\"\"\"\n",
    "    def loss_fn(model):\n",
    "        logits = model(padded_audios, mask=mask, training=True)\n",
    "        \n",
    "        logit_paddings = (jnp.arange(logits.shape[1]) >= real_times[:, None]).astype(jnp.float32)\n",
    "        label_paddings = (jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]).astype(jnp.float32)\n",
    "        \n",
    "        loss = optax.ctc_loss(logits, logit_paddings, padded_labels, label_paddings, blank_id=tokenizer.blank_id).mean()\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "    return loss\n",
    "\n",
    "@nnx.jit\n",
    "def jitted_eval(model, padded_audios, padded_labels, mask, real_times, label_lengths):\n",
    "    \"\"\"Evaluation step - no gradient computation\"\"\"\n",
    "    logits = model(padded_audios, mask=mask, training=False)\n",
    "    \n",
    "    logit_paddings = (jnp.arange(logits.shape[1]) >= real_times[:, None]).astype(jnp.float32)\n",
    "    label_paddings = (jnp.arange(padded_labels.shape[1]) >= label_lengths[:, None]).astype(jnp.float32)\n",
    "    \n",
    "    loss = optax.ctc_loss(logits, logit_paddings, padded_labels, label_paddings, blank_id=tokenizer.blank_id).mean()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def run_validation(model, val_dataset):\n",
    "    \"\"\"Run validation and return average loss\"\"\"\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for element in val_dataset:\n",
    "        padded_audios, frames, padded_labels, label_lengths = element\n",
    "        mask, real_times = compute_mask(frames)\n",
    "        loss = jitted_eval(model, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "        total_loss += float(loss)\n",
    "        num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "\n",
    "# Training configuration\n",
    "NUM_EPOCHS = 1\n",
    "VAL_EVERY_N_STEPS = 500  # Run validation every N steps (set to None to validate only at epoch end)\n",
    "global_step = checkpointer.latest_step() or 0\n",
    "\n",
    "print(f\"Starting training from step {global_step}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training loop\n",
    "    train_loss_sum = 0.0\n",
    "    train_steps = 0\n",
    "    \n",
    "    pbar = tqdm(processed_train_dataset, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    for element in pbar:\n",
    "        padded_audios, frames, padded_labels, label_lengths = element\n",
    "        mask, real_times = compute_mask(frames)\n",
    "        \n",
    "        loss = jitted_train(model, optimizer, padded_audios, padded_labels, mask, real_times, label_lengths)\n",
    "        \n",
    "        train_loss_sum += float(loss)\n",
    "        train_steps += 1\n",
    "        global_step += 1\n",
    "        \n",
    "        # Update tqdm with running average loss\n",
    "        avg_train_loss = train_loss_sum / train_steps\n",
    "        pbar.set_postfix({\"train_loss\": f\"{avg_train_loss:.2f}\", \"step\": global_step})\n",
    "        \n",
    "        # Optional: mid-epoch validation\n",
    "        if VAL_EVERY_N_STEPS and global_step % VAL_EVERY_N_STEPS == 0:\n",
    "            val_loss = run_validation(model, processed_test_dataset)\n",
    "            pbar.set_postfix({\"train_loss\": f\"{avg_train_loss:.2f}\", \"val_loss\": f\"{val_loss:.2f}\", \"step\": global_step})\n",
    "    \n",
    "    # End of epoch validation\n",
    "    val_loss = run_validation(model, processed_test_dataset)\n",
    "    avg_train_loss = train_loss_sum / train_steps if train_steps > 0 else 0.0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1} complete - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoint after each epoch\n",
    "    checkpointer.save(\n",
    "        global_step,\n",
    "        args=ocp.args.Composite(\n",
    "            model=ocp.args.StandardSave(nnx.state(model)),\n",
    "            optimizer=ocp.args.StandardSave(nnx.state(optimizer)),\n",
    "        )\n",
    "    )\n",
    "    print(f\"Checkpoint saved at step {global_step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
