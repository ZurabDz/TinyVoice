{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e012eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from conformer.tokenizer import Tokenizer\n",
    "from conformer.dataset import batch_fn, ProcessAudioData, unpack_speech_data\n",
    "import grain\n",
    "from functools import partial\n",
    "from conformer.conformer_block import ConformerEncoder\n",
    "from conformer.config import ConformerConfig, TrainingConfig\n",
    "from flax import nnx\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d414c91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "conformer_config = ConformerConfig()\n",
    "train_config = TrainingConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab8f7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.load('/home/penguin/data/tokenizer/tokenizer.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00f9ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_source = grain.sources.ArrayRecordDataSource('/home/penguin/data/packed_dataset/data.array_record')\n",
    "with_tokenizer_batch_fn = partial(batch_fn, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b4f6e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_audio_dataset = grain.MapDataset.source(audio_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3a069cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_datasets = (\n",
    "    map_audio_dataset\n",
    "    .shuffle(seed=42)\n",
    "    .map(ProcessAudioData(tokenizer))\n",
    "    .batch(batch_size=24, batch_fn=with_tokenizer_batch_fn)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7118bfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConformerEncoder(conformer_config, num_classes=42, rngs=nnx.Rngs(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7d31ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(lengths: jnp.ndarray, max_len: int) -> jnp.ndarray:\n",
    "    batch_size = lengths.shape[0]\n",
    "    indices = jnp.arange(max_len).reshape(1, -1)\n",
    "    mask = indices >= lengths.reshape(-1, 1)\n",
    "    return mask.astype(jnp.float32)\n",
    "\n",
    "def create_learning_rate_fn(warmup_steps: int, model_size: int):\n",
    "    def lr_fn(step):\n",
    "        arg1 = 1 / jnp.sqrt(step + 1e-9)\n",
    "        arg2 = step * (warmup_steps ** -1.5)\n",
    "        return (1 / jnp.sqrt(model_size)) * jnp.minimum(arg1, arg2)\n",
    "    return lr_fn\n",
    "\n",
    "@nnx.jit(donate_argnums=0)\n",
    "def train_step(model: ConformerEncoder, optimizer: nnx.Optimizer, batch: dict):\n",
    "\n",
    "    def loss_fn(model: ConformerEncoder):\n",
    "        log_probs, output_lengths = model(\n",
    "            batch[\"inputs\"], batch[\"input_lengths\"], training=True\n",
    "        )\n",
    "\n",
    "        max_logit_len = log_probs.shape[1]\n",
    "        max_label_len = batch[\"labels\"].shape[1]\n",
    "        logit_paddings = create_padding_mask(output_lengths, max_logit_len)\n",
    "        label_paddings = create_padding_mask(batch[\"label_lengths\"], max_label_len)\n",
    "        \n",
    "        loss = optax.ctc_loss(\n",
    "            log_probs, logit_paddings, batch[\"labels\"], label_paddings\n",
    "        ).mean()\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
    "    optimizer.update(model=model, grads=grads)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7db1049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = create_learning_rate_fn(train_config.warmup_steps, conformer_config.encoder_dim)\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(\n",
    "        learning_rate=lr_schedule,\n",
    "        b1=train_config.beta1,\n",
    "        b2=train_config.beta2,\n",
    "        weight_decay=train_config.weight_decay,\n",
    "    ),\n",
    "    wrt=nnx.Param\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e826df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = train_step(model, optimizer, example_datasets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "793a3b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 19/4858 [00:07<33:39,  2.40it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 20: Average Loss = 908.2094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_loss_accumulator = 0\n",
    "step_count = 0\n",
    "n_steps_to_print_avg_loss = 20\n",
    "\n",
    "\n",
    "for batch in tqdm(example_datasets):\n",
    "    loss = train_step(model, optimizer, batch)\n",
    "    total_loss_accumulator += loss.item()\n",
    "    step_count += 1\n",
    "\n",
    "    if step_count % n_steps_to_print_avg_loss == 0:\n",
    "        average_loss = total_loss_accumulator / n_steps_to_print_avg_loss\n",
    "        print(f\"Step {step_count}: Average Loss = {average_loss:.4f}\")\n",
    "        \n",
    "        # Reset the accumulator and counter for the next interval\n",
    "        total_loss_accumulator = 0.0\n",
    "        step_count = 0\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1180e223",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs, output_lengths = model(\n",
    "    example_datasets[0][\"inputs\"], example_datasets[0][\"input_lengths\"], training=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f87afa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "226b6e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5b77aba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, batch, tokenizer_decode):\n",
    "    \"\"\"\n",
    "    Evaluate a batch of inputs using the model and compute WER.\n",
    "\n",
    "    Args:\n",
    "        model: The Flax model (nnx.Module) to evaluate.\n",
    "        batch: Dictionary containing 'inputs', 'input_lengths', and 'labels'.\n",
    "        tokenizer_decode: Callable that decodes a single sequence of token IDs to a string.\n",
    "\n",
    "    Returns:\n",
    "        WER score for the batch.\n",
    "    \"\"\"\n",
    "    # Forward pass through the model\n",
    "    log_probs, output_lengths = model(\n",
    "        batch[\"inputs\"], batch[\"input_lengths\"], training=False\n",
    "    )\n",
    "\n",
    "    # Get predicted token IDs\n",
    "    prediction_tokens = jnp.argmax(log_probs, axis=-1)\n",
    "\n",
    "    # Vectorized decoding using vmap\n",
    "    predictions = jax.vmap(lambda x: np.array(tokenizer_decode(x)))(prediction_tokens)\n",
    "    real_values = jax.vmap(lambda x: np.array(tokenizer_decode(x)))(batch[\"labels\"])\n",
    "\n",
    "    # Convert JAX arrays to Python lists for WER computation\n",
    "    predictions = predictions.tolist()\n",
    "    real_values = real_values.tolist()\n",
    "\n",
    "    return wer(predictions, real_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "38d3b8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @nnx.jit(donate_argnums=0, static_argnames=('tokenizer'))\n",
    "def eval_step(model, batch, tokenizer):\n",
    "    log_probs, output_lengths = model(\n",
    "        batch[\"inputs\"], batch[\"input_lengths\"], training=False\n",
    "    )\n",
    "\n",
    "    prediction_tokens = jnp.argmax(log_probs, axis=-1)\n",
    "    predictions = []\n",
    "    for element in prediction_tokens:\n",
    "        predictions.append(tokenizer.decode(element.tolist()))\n",
    "\n",
    "    real_values = []\n",
    "    for element in batch['labels']:\n",
    "        real_values.append(tokenizer.decode(element.tolist()))\n",
    "\n",
    "    return wer(predictions, real_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "64a192e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "133"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_step(model, batch, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bbc7f074",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = jnp.argmax(log_probs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ce6347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in predictions:\n",
    "    text = tokenizer.decode(element.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1b720649",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in example_datasets[0]['labels']:\n",
    "    text = tokenizer.decode(element.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0185714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "19195736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer(text, ' მდგომარეობა პოლონეთში გავრცელების არეალის ძირითადი მონაკვეთია')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0928b696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ამ მხრივ უკეთესი მდგომარეობა პოლონეთში გავრცელების არეალის ძირითადი მონაკვეთია რუსეთი სკანდინავია მათ შორის ფინეთი'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c45d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-4, -4.5625, -1.67969, -3.9375, -3.125, -4.375, -4.1875, -3.01562,\n",
       "       -5.9375, -4.65625, -3.79688, -5.8125, -3, -3.5, -3.45312, -3.70312,\n",
       "       -3.65625, -3.875, -3.01562, -4.40625, -3.84375, -4.8125, -3.96875,\n",
       "       -4.65625, -4.5625, -5.1875, -4.375, -5, -5.71875, -5.375, -4.71875,\n",
       "       -4.125, -3.09375, -4.875, -3.26562, -5.46875, -3.1875, -4.8125,\n",
       "       -5.59375, -3.78125, -3.125, -3.8125], dtype=bfloat16)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "log_probs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd7e251",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(model, batch, tokenizer):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
